{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. read to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "memb_info = pd.read_csv('/data/user/workspace/model-irt/memb_info_v1.0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. preprocessing for the generation of derivative variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 변수만을 추출하여 clustering용 data set 지정\n",
    "memb_info = memb_info[['학년',\n",
    "                       '완료학습개수', '출석횟수',\n",
    "                       '국어_평균_점수', '영어_평균_점수', '수학_평균_점수', '사회_평균_점수', '과학_평균_점수',\n",
    "                       '평균출석율', '계획완료개수_코스', '계획완료개수_차시추가', '비계획완료', '국어_오답_수행율', '영어_오답_수행율', '수학_오답_수행율', '사회_오답_수행율', '과학_오답_수행율',\n",
    "                       '주중출석횟수', '강의타입:(영상+문제)_수', '강의타입:첨삭_수', '강의타입:문제_수', '강의타입:영상_수', '강의타입:T_ABILITY_EVAL_SS_OL없음_수', '강의타입:CORE없음_수', '강의타입:WHY없음_수', '강의타입:T_PENG없음_수']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memb_info['주말출석율'] = (memb_info['출석횟수'] - memb_info['주중출석횟수']) / memb_info['출석횟수']\n",
    "memb_info['주중출석율'] = memb_info['주중출석횟수'] / memb_info['출석횟수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 강의타입:XX_수 / 완료학습개수\n",
    "# 단, T_ABILITY_EVAL_SS_OL없음_수, CORE없음_수, WHY없음_수, T_PENG없음_수는\n",
    "# null이 매우 많기에 날림\n",
    "# 날리는 방식에 있어서는 해당 학생의 완료학습개수를 빼줌\n",
    "# 위에서 확인한 결과 null이 될 경우를 고려 안 해도 될 듯\n",
    "\n",
    "del_list = ['강의타입:T_ABILITY_EVAL_SS_OL없음_수', '강의타입:CORE없음_수', '강의타입:WHY없음_수', '강의타입:T_PENG없음_수']\n",
    "\n",
    "user_list = []\n",
    "\n",
    "for type in del_list:\n",
    "    print(\"type name: {}\".format(type))\n",
    "    for row in memb_info['{}'.format(type)].dropna().iteritems():\n",
    "#         print(\"pre_완료학습개수: {}\".format(origin_set['완료학습개수'][row[0]]))\n",
    "#         print(\"abstract value: {}\".format(row[1]))\n",
    "        memb_info['완료학습개수'][row[0]] = memb_info['완료학습개수'][row[0]] - row[1]\n",
    "        \n",
    "#         print(\"post_완료학습개수: {}\".format(origin_set['완료학습개수'][row[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 강의타입:XX_수 / 완료학습개수\n",
    "\n",
    "type_list = ['강의타입:(영상+문제)_수', '강의타입:첨삭_수', '강의타입:문제_수', '강의타입:영상_수']\n",
    "\n",
    "for type in type_list:\n",
    "    memb_info['타입율:{}'.format(type[5:])] = memb_info[type] / memb_info['완료학습개수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출석횟수 대비 완료학습\n",
    "memb_info['일일_공부량'] = memb_info['완료학습개수'] / memb_info['출석횟수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계획완료율\n",
    "memb_info['계획완료율'] = (memb_info['계획완료개수_코스'] + memb_info['계획완료개수_차시추가']) / memb_info['완료학습개수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비계획완료\n",
    "memb_info['비계획완료율'] = memb_info['비계획완료'] / memb_info['완료학습개수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memb_info = memb_info[['학년',\n",
    "                       '일일_공부량',\n",
    "                       '국어_평균_점수', '영어_평균_점수', '수학_평균_점수', '사회_평균_점수', '과학_평균_점수',\n",
    "                       '계획완료율', '비계획완료율', '국어_오답_수행율', '영어_오답_수행율', '수학_오답_수행율', '사회_오답_수행율', '과학_오답_수행율',\n",
    "                       '평균출석율']]\n",
    "memb_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove row with outlier\n",
    "z_scores = stats.zscore(memb_info, axis=1)\n",
    "z_scores\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3.5).all(axis=1)\n",
    "memb_info = memb_info[filtered_entries]\n",
    "memb_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "mms_values = mms.fit_transform(memb_info.values)\n",
    "mms_set = pd.DataFrame(mms_values, columns=memb_info.columns)\n",
    "mms_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# define of the data set\n",
    "X = mms_set\n",
    "\n",
    "# define of the n_cluster\n",
    "range_n_clusters = list(range(3, 16))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # Silhouette_score \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    \n",
    "    # Davies_Bouldin_socre \n",
    "    davies_avg = davies_bouldin_score(X, cluster_labels)\n",
    "    \n",
    "    # Calinski_harabasz_score\n",
    "    calinski_avg = calinski_harabasz_score(X, cluster_labels)\n",
    "    \n",
    "    print(\"for n_clusters = {} | silhouettet_socre = {} | davies_score = {} | calinski_score = {}\".format(n_clusters, round(silhouette_avg, 4), round(davies_avg, 4), round(calinski_avg, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization for Elbow\n",
    "ks = range(1, 15)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# define of the data set\n",
    "X = mms_set\n",
    "\n",
    "# define of the n_cluster\n",
    "range_n_clusters = list(range(3, 16))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage='average') # linkage = 'average'\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # Silhouette_score \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    \n",
    "    # Davies_Bouldin_socre \n",
    "    davies_avg = davies_bouldin_score(X, cluster_labels)\n",
    "    \n",
    "    # Calinski_harabasz_score\n",
    "    calinski_avg = calinski_harabasz_score(X, cluster_labels)\n",
    "    \n",
    "    print(\"for n_clusters = {} | silhouettet_socre = {} | davies_score = {} | calinski_score = {}\".format(n_clusters, round(silhouette_avg, 4), round(davies_avg, 4), round(calinski_avg, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# define of the data set\n",
    "X = mms_set\n",
    "\n",
    "# define of the n_cluster\n",
    "range_n_clusters = list(range(3, 16))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage='complete') # linkage = 'complete'\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # Silhouette_score \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    \n",
    "    # Davies_Bouldin_socre \n",
    "    davies_avg = davies_bouldin_score(X, cluster_labels)\n",
    "    \n",
    "    # Calinski_harabasz_score\n",
    "    calinski_avg = calinski_harabasz_score(X, cluster_labels)\n",
    "    \n",
    "    print(\"for n_clusters = {} | silhouettet_socre = {} | davies_score = {} | calinski_score = {}\".format(n_clusters, round(silhouette_avg, 2), round(davies_avg, 2), round(calinski_avg, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# define of the data set\n",
    "X = mms_set\n",
    "\n",
    "for min_samples in range(5, 21):\n",
    "    for eps in range(5, 20):\n",
    "        try:\n",
    "            if eps < 10:\n",
    "                # min_samples & eps\n",
    "                print(\"############################################\")\n",
    "                print(\"min_samples: {} | eps: 0.{}\".format(min_samples, eps))\n",
    "\n",
    "                # Initialize the clusterer with n_clusters value and a random generator\n",
    "                dbscan = DBSCAN(min_samples=min_samples, n_jobs=-1, eps=float(\"0.{}\".format(eps)))\n",
    "                dbscan_labels = dbscan.fit_predict(pd.DataFrame(X, columns=stdsc_set.columns))\n",
    "\n",
    "                # silhouette_score \n",
    "                silhouette_avg = silhouette_score(X, dbscan_labels)\n",
    "                print(\"For n_clusters =\", len(pd.Series(dbscan_labels).unique()),\n",
    "                    \"The silhouette_score is :\", round(silhouette_avg, 2))\n",
    "\n",
    "                # Davies_Bouldin_socre \n",
    "                davies_avg = davies_bouldin_score(X, dbscan_labels)\n",
    "                print(\"For n_clusters =\", len(pd.Series(dbscan_labels).unique()),\n",
    "                    \"The davies_bouldin_score is :\", round(davies_avg, 2))\n",
    "\n",
    "                # Calinski_harabasz_score\n",
    "                calinski_avg = calinski_harabasz_score(X, dbscan_labels)\n",
    "                print(\"For n_clusters =\", len(pd.Series(dbscan_labels).unique()),\n",
    "                    \"The calinsk_harabasz_socre is :\", round(calinski_avg, 2))\n",
    "\n",
    "                print(\"############################################\\n\")\n",
    "            else:\n",
    "                # min_samples & eps\n",
    "                print(\"############################################\")\n",
    "                print(\"min_samples: {} | eps: {}.{}\".format(min_samples, str(eps)[0], str(eps)[1]))\n",
    "\n",
    "                # Initialize the clusterer with n_clusters value and a random generator\n",
    "                dbscan = DBSCAN(min_samples=min_samples, n_jobs=-1, eps=float(\"{}.{}\".format(str(eps)[0], str(eps)[1])))\n",
    "                dbscan_labels = dbscan.fit_predict(pd.DataFrame(X, columns=stdsc_set.columns))\n",
    "\n",
    "                # silhouette_score \n",
    "                silhouette_avg = silhouette_score(X, dbscan_labels)\n",
    "                print(\"For n_clusters =\", len(pd.Series(dbscan_labels).unique()),\n",
    "                    \"The silhouette_score is :\", round(silhouette_avg, 2))\n",
    "\n",
    "                # Davies_Bouldin_socre \n",
    "                davies_avg = davies_bouldin_score(X, dbscan_labels)\n",
    "                print(\"For n_clusters =\", len(pd.Series(dbscan_labels).unique()),\n",
    "                    \"The davies_bouldin_score is :\", round(davies_avg, 2))\n",
    "\n",
    "                # Calinski_harabasz_score\n",
    "                calinski_avg = calinski_harabasz_score(X, dbscan_labels)\n",
    "                print(\"For n_clusters =\", len(pd.Series(dbscan_labels).unique()),\n",
    "                    \"The calinsk_harabasz_socre is :\", round(calinski_avg, 2))\n",
    "\n",
    "                print(\"############################################\\n\")\n",
    "                \n",
    "        # DBSCAN parameter인 min_samples, eps에 따라 군집형성이 안 될 수 있기 때문에\n",
    "        # 군집 형성 자체가 안 되었다면 실루엣 지수를 구할 수 없고\n",
    "        # 이에 따라서 ValueError가 발생\n",
    "        except ValueError:\n",
    "            print(\"ValueError... for n_clusters = {}\".format(len(pd.Series(dbscan_labels).unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import SimpSOM as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms_set = np.array(mms_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a network 20x20 with a weights format taken from the raw_data and activate Periodic Boundary Conditions. \n",
    "net = sps.somNet(10, 10, mms_set, PBC=True, PCI=True)\n",
    "\n",
    "# Train the network for 10000 epochs and with initial learning rate of 0.01. \n",
    "net.train(0.01, 10000)\n",
    "\n",
    "# Project the datapoints on the new 2D network map\n",
    "prj = np.array(net.project(pd.DataFrame(mms_set).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a map of the network nodes and colour them according to the first feature (column number 0) of the dataset\n",
    "# and then according to the distance between each node and its neighbours.\n",
    "# this will help us identify cluster centers \n",
    "net.diff_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# define of the data set\n",
    "X = prj\n",
    "\n",
    "# define of the n_cluster\n",
    "range_n_clusters = list(range(3, 15))\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # Silhouette_score \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    \n",
    "    # Davies_Bouldin_socre \n",
    "    davies_avg = davies_bouldin_score(X, cluster_labels)\n",
    "    \n",
    "    # Calinski_harabasz_score\n",
    "    calinski_avg = calinski_harabasz_score(X, cluster_labels)\n",
    "    \n",
    "    print(\"for n_clusters = {} | silhouettet_socre = {} | davies_score = {} | calinski_score = {}\".format(n_clusters, round(silhouette_avg, 4), round(davies_avg, 4), round(calinski_avg, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-irt",
   "language": "python",
   "name": "model-irt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
