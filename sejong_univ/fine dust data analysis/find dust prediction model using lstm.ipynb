{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.io import sql\n",
    "from sqlalchemy import create_engine\n",
    "from matplotlib.pyplot import savefig\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collect\n",
    "engine = create_engine('')\n",
    "cnn = engine.connect()\n",
    "\n",
    "data = sql.read_sql(\"\", cnn)\n",
    "data.columns = ['idx','Location','Date','SO2','CO','O3','NO2','PM10','PM25']\n",
    "data = pd.DataFrame(data,columns=['Location','Date','PM10'])\n",
    "data_seoul=data[data.Location.isin(['서울'])]\n",
    "data_seoul=data_seoul.sort_values(['Date'], ascending=[True])\n",
    "del data_seoul['Location']\n",
    "data_seoul=data_seoul.set_index(['Date'])\n",
    "print(len(data_seoul))\n",
    "data_seoul=pd.rolling_mean(data_seoul,30)\n",
    "data_seoul=data_seoul[29:]\n",
    "\n",
    "data_seoul_train=data_seoul[:2864]\n",
    "data_seoul_test=data_seoul[2864:]\n",
    "\n",
    "# RNN Example\n",
    "PM10_train = np.round(data_seoul_train.PM10.tolist(), 3)\n",
    "PM10_test = np.round(data_seoul_test.PM10.tolist(), 3)\n",
    "\n",
    "from scipy.linalg import toeplitz\n",
    "# 3차원 텐서를 생성\n",
    "devision_train=np.fliplr(toeplitz(np.r_[PM10_train[-1], np.zeros(PM10_train.shape[0] - 2)], PM10_train[::-1]))\n",
    "devision_test=np.fliplr(toeplitz(np.r_[PM10_test[-1], np.zeros(PM10_test.shape[0] - 2)], PM10_train[::-1]))\n",
    "\n",
    "# 30일기준으로 MA를 하였으니 30일로 일단 실행\n",
    "X_train = devision_train[:-1, :10][:,:,np.newaxis]\n",
    "X_test = devision_test[:-1, :10][:,:,np.newaxis]\n",
    "\n",
    "Y_train = devision_train[:-1, 10]\n",
    "Y_test = devision_test[:-1, 10]\n",
    "\n",
    "# 30일 MA의 전체 length는 3621이니 3621을 30일치씩 실행\n",
    "#print(X_train.shape, Y_train.shape)\n",
    "\n",
    "# X의 값이 30일치이고 Y의 값은 그 다음에 나오는 값\n",
    "# X의 값 30일로 Y의 1번째의 값을 구하는 구도\n",
    "#print(\"X의 training값\",X_train[:2])\n",
    "#print(\"Y의 training값\",Y_train[:2])\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6)) \n",
    "plt.subplot(211)\n",
    "plt.plot([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], X_train[0].flatten(), 'bo-', label=\"input sequence\")\n",
    "plt.plot([10], Y_train[0], 'ro', label=\"target\")\n",
    "plt.xlim(0,13)\n",
    "plt.ylim(60,70)\n",
    "plt.legend()\n",
    "plt.title(\"First sample sequence\")\n",
    "plt.subplot(212)\n",
    "plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], X_train[1].flatten(), 'bo-', label=\"input sequence\")\n",
    "plt.plot([11], Y_train[1], 'ro', label=\"target\")\n",
    "plt.xlim(0,13)\n",
    "plt.ylim(60,70)\n",
    "plt.legend()\n",
    "plt.title(\"Second sample sequence\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Test.jpg\", dpi=500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seoul.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=15)\n",
    "matplotlib.rc('ytick', labelsize=15)\n",
    "matplotlib.rc('legend', fontsize=15)\n",
    "\n",
    "data_seoul=data[data.Location.isin(['서울'])]\n",
    "data_seoul=data_seoul.sort_values(['Date'], ascending=[True])\n",
    "del data_seoul['Location']\n",
    "data_seoul=data_seoul.set_index(['Date'])\n",
    "\n",
    "data_seoul_re=pd.rolling_mean(data_seoul,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=data_seoul.plot(figsize=(16,10))\n",
    "ax.set_xticks(np.arange(0,4107,365))\n",
    "\n",
    "date_range = pd.date_range('2005', '2017', freq='A')\n",
    "date_range = date_range.map(lambda t: t.strftime('%Y'))\n",
    "ax.set_xticklabels(date_range)\n",
    "\n",
    "ax.set_ylim(0,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=data_seoul_re.plot(figsize=(16,10))\n",
    "ax.set_xticks(np.arange(0,4107,365))\n",
    "\n",
    "date_range = pd.date_range('2005', '2017', freq='A')\n",
    "date_range = date_range.map(lambda t: t.strftime('%Y'))\n",
    "ax.set_xticklabels(date_range)\n",
    "ax.set_ylim(0,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Example Air Quality using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('')\n",
    "cnn = engine.connect()\n",
    "\n",
    "data = sql.read_sql(\"\", cnn)\n",
    "data.columns = ['idx','Location','Date','SO2','CO','O3','NO2','PM10','PM25']\n",
    "#data = pd.DataFrame(data,columns=['Location','Date','PM10'])\n",
    "data = pd.DataFrame(data,columns=['Location','Date','PM10','SO2','CO','O3','NO2'])\n",
    "data_seoul=data[data.Location.isin(['서울'])]\n",
    "data_seoul=data_seoul.sort_values(['Date'], ascending=[True])\n",
    "del data_seoul['Location']\n",
    "data_seoul=data_seoul.set_index(['Date'])\n",
    "#print(len(data_seoul))\n",
    "#print(data_seoul)\n",
    "data_seoul=pd.rolling_mean(data_seoul,30)\n",
    "data_seoul=data_seoul[29:]\n",
    "\n",
    "# test & train set\n",
    "dataset=data_seoul.values\n",
    "dataset = np.round(dataset.astype('float32'),3)\n",
    "\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "#train_size = int(len(dataset) * 0.8) #원본\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "look_back = 14\n",
    "#look_back = 14\n",
    "\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "print(trainX)\n",
    "print(trainY)\n",
    "\n",
    "# callback loss history set\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "batch_size=7\n",
    "#batch_size=7\n",
    "history = LossHistory()        \n",
    "model = Sequential()\n",
    "#model.add(Dense(8, input_dim=look_back, activation='relu'))\n",
    "model.add(Dense(1, input_dim=look_back, activation='relu'))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.fit(trainX, trainY, nb_epoch=50, batch_size=batch_size, callbacks=[history], validation_data=(testX, testY), shuffle=False)\n",
    "\n",
    "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(testX, testY, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))\n",
    "\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "print(len(trainPredict))\n",
    "print(len(testPredict))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "fig=plt.figure(1,figsize=(16,10))\n",
    "\n",
    "plt.plot(dataset, color='blue')\n",
    "dataset_legend=[\"dataset\"]\n",
    "plt.legend(dataset_legend)\n",
    "plt.plot(trainPredictPlot, color='green')\n",
    "plt.plot(testPredictPlot, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.io import sql\n",
    "from sqlalchemy import create_engine\n",
    "from matplotlib.pyplot import savefig\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Data Collect\n",
    "engine = create_engine('mysql+pymysql://jh:jh@211.180.114.142/armway_db?charset=utf8')\n",
    "cnn = engine.connect()\n",
    "\n",
    "data = sql.read_sql(\"select * from 20161122_day_air_quality\", cnn)\n",
    "data.columns = ['idx','Location','Date','SO2','CO','O3','NO2','PM10','PM25']\n",
    "data = pd.DataFrame(data,columns=['Location','Date','PM10','O3','NO2','CO','SO2'])\n",
    "data_seoul=data[data.Location.isin(['서울'])]\n",
    "data_seoul=data_seoul.sort_values(['Date'], ascending=[True])\n",
    "del data_seoul['Location']\n",
    "data_seoul=data_seoul.set_index(['Date'])\n",
    "data_seoul=pd.rolling_mean(data_seoul,30)\n",
    "data_seoul=data_seoul[29:]\n",
    "\n",
    "data_seoul_train=data_seoul[:3263]\n",
    "data_seoul_test=data_seoul[3264:]\n",
    "\n",
    "# RNN Example\n",
    "PM10_train = np.round(data_seoul_train.PM10.tolist(), 3)\n",
    "PM10_test = np.round(data_seoul_test.PM10.tolist(), 3)\n",
    "\n",
    "from scipy.linalg import toeplitz\n",
    "# 3차원 텐서를 생성\n",
    "# feature는 1, 30개의 데이터\n",
    "devision_train=np.fliplr(toeplitz(np.r_[PM10_train[-1], np.zeros(PM10_train.shape[0] - 2)], PM10_train[::-1]))\n",
    "devision_test=np.fliplr(toeplitz(np.r_[PM10_test[-1], np.zeros(PM10_test.shape[0] - 2)], PM10_train[::-1]))\n",
    "\n",
    "# 30일기준으로 MA를 하였으니 30일로 일단 실행\n",
    "X_train = devision_train[:-1, :30][:,:,np.newaxis]\n",
    "X_test = devision_test[:-1, :30][:,:,np.newaxis]\n",
    "\n",
    "# 역순으로 입력\n",
    "Y_train = devision_train[:-1, 30]\n",
    "Y_test = devision_test[:-1, 30]\n",
    "\n",
    "# 30일 MA의 전체 length는 3621이니 3621을 30일치씩 실행\n",
    "print(X_train.shape, Y_train.shape)\n",
    "# X의 값이 30일치이고 Y의 값은 그 다음에 나오는 값\n",
    "# X의 값 30일로 Y의 1번째의 값을 구하는 구도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test), len(Y_test))\n",
    "print(len(X_train), len(Y_train))\n",
    "print(len(X_test)+len(X_train))\n",
    "print(len(data_seoul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature는 1개 미세먼지만 TimeSeries Feature이기때문\n",
    "len(X_train[3260])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "history = LossHistory() \n",
    "\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 500\n",
    "length_of_sequences = 30\n",
    "model = Sequential()  \n",
    "model.add(LSTM(hidden_neurons, batch_input_shape=(None, length_of_sequences, in_out_neurons), return_sequences=False))  \n",
    "model.add(Dense(in_out_neurons))  \n",
    "model.add(Activation(\"relu\")) \n",
    "#model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\",metrics=['accuracy'])\n",
    "model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "\n",
    "model.fit(X_train,Y_train, batch_size=30, nb_epoch=50, validation_split=0.03, callbacks=[history]) \n",
    "#model.fit(X_train,Y_train, batch_size=7, nb_epoch=50, validation_split=0.03, callbacks=[history]) \n",
    "\n",
    "trainScore = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=15)\n",
    "matplotlib.rc('ytick', labelsize=15)\n",
    "matplotlib.rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[0].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.plot(history.losses[0:400])\n",
    "plt.legend([\"LSTM_Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "print(len(trainPredict))\n",
    "print(len(testPredict))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "fig=plt.figure(1,figsize=(16,10))\n",
    "\n",
    "plt.plot(dataset, color='blue')\n",
    "dataset_legend=[\"dataset\"]\n",
    "plt.legend(dataset_legend)\n",
    "plt.plot(trainPredictPlot, color='green')\n",
    "plt.plot(testPredictPlot, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "trainPredict = model.predict(X_train)\n",
    "testPredict = model.predict(X_test)\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "fig=plt.figure(1,figsize=(16,10))\n",
    "\n",
    "plt.plot(dataset, color='blue')\n",
    "dataset_legend=[\"dataset\"]\n",
    "plt.legend(dataset_legend)\n",
    "plt.plot(trainPredictPlot, color='green')\n",
    "plt.plot(testPredictPlot, color='red')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
